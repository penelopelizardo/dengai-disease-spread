# -*- coding: utf-8 -*-
"""[Entregado] 07MBID Plantilla - Práctica 1 - Penélope Lizardo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_vrpGLCZCeDBv6nU4OgnjOy7seqv4wPD

# <center>PRÁCTICA 1: APRENDIZAJE NO SUPERVISADO (DengAI)<center>

**Nombre y apellidos:** Penélope Lizardo Valenzuela

**Usuario VIU:** plizardov@student.universidadviu.com

---
# Resumen
---

En esta práctica de machine learning, me propongo explorar el aprendizaje no supervisado usando técnicas de clustering sobre el dataset de **DengAI: Predicting Disease Spread** de DrivenData. A través de este análisis, quiero encontrar patrones en los datos que nos permitan entender mejor la propagación del dengue en diferentes regiones, considerando factores ambientales y condiciones climáticas.

### Fases de mi experimentación

He dividido la experimentación en varias fases clave para estructurar bien el proceso y sacar el máximo provecho de los datos:

1. **Preprocesamiento de datos**: Comenzaré limpiando el dataset, lo que implica manejar valores nulos, detectar y tratar los outliers, y binarizar la variable `city` para simplificar el análisis. También aprovecharé para generar algunas características adicionales que podrían resultar útiles y hacer un análisis estadístico para identificar las variables más relevantes.

2. **Reducción de dimensionalidad**: Para hacer más manejable la visualización de los resultados y facilitar el análisis, aplicaré técnicas como PCA (Análisis de Componentes Principales). Esto me permitirá reducir el número de dimensiones y proyectar los datos en 2D o 3D, de modo que los clusters resultantes sean más fáciles de interpretar.

3. **Clustering**: Aquí es donde probaré diferentes técnicas de clustering, como K-means, clustering jerárquico, DBSCAN y MeanShift. Cada técnica me ayudará a agrupar los datos de una forma distinta, y quiero ver cuál de ellas es más efectiva en capturar patrones que reflejen las condiciones ambientales y geográficas en el contexto de la propagación del dengue.

4. **Evaluación de resultados**: Finalmente, compararé los clusters generados por cada método usando visualizaciones y métricas específicas. Así podré entender mejor cuál de los métodos logra una agrupación más significativa y útil para interpretar la influencia de factores como la temperatura, humedad y precipitación en la propagación de la enfermedad.

**Resultados que espero obtener**

Al concluir la experimentación, espero poder identificar los métodos de clustering que mejor agrupan los datos de acuerdo con los factores ambientales que influyen en la propagación del dengue. Mi objetivo es obtener insights que puedan ser útiles para estudios epidemiológicos y prevención, ayudándonos a ver cómo el clima y el entorno natural pueden afectar a estas enfermedades.

Para acceder a este dataset, es necesario registrarse en la competición de **DengAI: Predicting Disease Spread** de DrivenData, disponible en: [DengAI Predicting Disease Spread](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/).

---
# Inicialización
---
"""

# Imports generales
import pandas as pd
import io
from google.colab import files

seed = 42

def upload_files (index_fields=None):
  uploaded = files.upload()
  for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(
        name=fn, length=len(uploaded[fn])))
    df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')), index_col = index_fields)
    return df

# Subir el conjunto de entrenamiento sin variable objetivo (dengue_features_train.csv)
train = upload_files()
print(train.shape)
train.head()

"""---
# Exploración
---

En esta y las siguientes secciones, iré describiendo y ejecutando los distintos experimentos, alternando entre celdas de texto y código para facilitar la comprensión del análisis.

Primero, realizaré un análisis estadístico completo del dataset para tener una visión general de sus características y distribución. También se generarán, si es adecuado, características derivadas adicionales que puedan ser útiles en los experimentos. En esta sección, identificaré y analizaré los valores atípicos (outliers) y decidiré si es necesario eliminarlos para mejorar la calidad de los datos.

Una vez completado el análisis inicial, pasaré al preprocesamiento de los datos, limpiándolos y aplicando las transformaciones necesarias. Este paso incluirá todas las funciones y transformaciones que permitirán que los algoritmos de machine learning se apliquen eficazmente al dataset.

## **1. Información básica del dataset**
"""

# Mostrar información básica sobre el dataset
print(train.info())

# Obtener dimensiones del dataset
filas, columnas = train.shape
print(f"El dataset tiene {filas} instancias y {columnas} columnas.")

"""Podemos visualizar que el dataset contiene 1456 instancias y 24 columnas de las cuales 22 son numéricas y 2 son categóricas.

Las características recogidas hacen referencia a:

1. **Datos de Vegetación Geográficamente**
* **ndvi_ne** : NDVI (Índice de Vegetación de Diferencia Normalizada) en el píxel al * noreste del centroide de la ciudad.
* **ndvi_nw**: NDVI en el píxel al noroeste del centroide de la ciudad.
* **ndvi_se**: NDVI en el píxel al sureste del centroide de la ciudad.
* **ndvi_sw**: NDVI en el píxel al suroeste del centroide de la ciudad.
2. **Datos Relacionados con las Precipitaciones**
* **station_precip_mm**: Precipitación total registrada en la estación meteorológica (en milímetros).
* **reanalysis_sat_precip_amt_mm**: Precipitación total medida por satélite (en milímetros).
* **reanalysis_precip_amt_kg_per_m2**: Precipitación total medida en unidades de masa por área (kilogramos por metro cuadrado).
* **precipitation_amt_mm**: Precipitación medida por satélite PERSIANN (en milímetros).
3. **Datos Relacionados con la Temperatura**
* **station_min_temp_c**: Temperatura mínima registrada en la estación (en grados Celsius).
* **station_max_temp_c**: Temperatura máxima registrada en la estación (en grados Celsius).
* **station_diur_temp_rng_c**: Rango de temperatura diurna en la estación (diferencia entre máxima y mínima, en grados Celsius).
* **station_avg_temp_c**: Temperatura promedio registrada en la estación (en grados Celsius).
* **reanalysis_min_air_temp_k**: Temperatura mínima del aire (en Kelvin) según reanálisis.
* **reanalysis_max_air_temp_k**: Temperatura máxima del aire (en Kelvin) según reanálisis.
* **reanalysis_dew_point_temp_k**: Temperatura del punto de rocío (en Kelvin).
* **reanalysis_avg_temp_k**: Temperatura promedio del aire (en Kelvin) según reanálisis.
* **reanalysis_air_temp_k**: Temperatura media del aire (en Kelvin).
* **reanalysis_tdtr_k**: Rango de temperatura diurna en el reanálisis (en Kelvin).
4. **Datos Relacionados con la Humedad**
* **reanalysis_specific_humidity_g_per_kg**: Humedad específica media (gramos de agua por kilogramo de aire).
* **reanalysis_relative_humidity_percent**: Humedad relativa media (en porcentaje).
5. **Indicadores de Ciudad y Fecha**
* **city**: Abreviatura de la ciudad donde se recopilan los datos:
sj para San Juan, Puerto Rico.
iq para Iquitos, Perú.
* **week_start_date**: Fecha de inicio de la semana en formato yyyy-mm-dd.
* **year**: Año en el que se registraron los datos. Esta variable es útil para identificar la tendencia temporal y analizar patrones anuales en la propagación de enfermedades o cambios climáticos.
* **weekofyear**: Número de la semana en el año en la que se registraron los datos (1 a 52). Este indicador permite analizar variaciones estacionales y estudiar patrones semanales.

## **2. Análisis estadístico del dataset**
En esta sección, voy a explorar a fondo el dataset para entender mejor sus características y preparar los datos para el análisis de clustering. Mi objetivo es obtener una visión general de cómo están distribuidos los datos, identificar posibles problemas como valores extremos (outliers) y aplicar las transformaciones necesarias para asegurar que el dataset esté listo para los algoritmos de machine learning.

Luego de rellenar los valores nulos, ejecutamos describe() para observar las estadísticas de las variables numéricas en el dataset..
"""

# Descripción estadística general de las variables numéricas
print(train.describe())

"""Al revisar los resultados estadísticos, noto que en la mayoría de las variables, la media y la mediana (percentil 50) están bastante cerca entre sí. Esto indica que las distribuciones son en su mayoría simétricas o tienen solo un ligero sesgo, lo cual es una buena señal para el análisis. Por ejemplo, en las variables relacionadas con la precipitación, las desviaciones estándar, como 43.78 para reanalysis_sat_precip_amt_mm y 47.36 para station_precip_mm, muestran una gran variabilidad, con valores máximos que son considerablemente más altos que el promedio. Esto sugiere la presencia de algunos valores atípicos, los cuales analizaré más adelante para decidir si conviene tratarlos.

En cuanto a las temperaturas, los valores de station_max_temp_c y station_min_temp_c están en rangos típicos de climas cálidos, con medias alrededor de 32°C y 22°C, respectivamente. Esto ayuda a tener una idea del tipo de clima en las áreas estudiadas. También observo que la variable reanalysis_relative_humidity_percent tiene una media alta de 82.17%, lo cual indica condiciones de alta humedad, otro factor relevante en el contexto del análisis ambiental que estamos realizando.

Estos detalles me ofrecen un contexto inicial sobre el clima y las condiciones ambientales de las áreas estudiadas, lo cual puede ser útil para interpretar los resultados de los algoritmos de clustering más adelante.

## **3. Observación de outliers potenciales**

He notado que algunas variables, como station_precip_mm y reanalysis_precip_amt_kg_per_m2, tienen valores máximos que se destacan en comparación con los cuartiles inferiores. Esto me sugiere la presencia de posibles outliers. Voy a profundizar en estos valores extremos para decidir si conviene eliminarlos o dejarlos, ya que podrían afectar los resultados de los algoritmos de clustering.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Visualización inicial de outliers usando boxplots para algunas variables clave
plt.figure(figsize=(12, 6))
sns.boxplot(data=train[['station_precip_mm', 'reanalysis_precip_amt_kg_per_m2']])
plt.title("Distribución de precipitaciones para detección de outliers")
plt.xlabel("Variables de Precipitación")
plt.ylabel("Valores")
plt.show()

"""En el boxplot de las variables de precipitación (station_precip_mm y reanalysis_precip_amt_kg_per_m2), podemos observar una cantidad significativa de outliers en ambos casos. Estos valores extremos se encuentran muy por encima del límite superior del rango intercuartílico (IQR), lo que sugiere que existen registros de precipitaciones inusualmente altas en algunas observaciones.

Para revisar los outliers en las variables de precipitación, vamos a identificarlos específicamente y ver algunos detalles sobre ellos.
"""

# Definir el rango intercuartílico (IQR) para las variables de precipitación
Q1 = train[['station_precip_mm', 'reanalysis_precip_amt_kg_per_m2']].quantile(0.25)
Q3 = train[['station_precip_mm', 'reanalysis_precip_amt_kg_per_m2']].quantile(0.75)
IQR = Q3 - Q1

# Definir los límites inferior y superior
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identificar los outliers
outliers = train[(train['station_precip_mm'] < lower_bound['station_precip_mm']) |
              (train['station_precip_mm'] > upper_bound['station_precip_mm']) |
              (train['reanalysis_precip_amt_kg_per_m2'] < lower_bound['reanalysis_precip_amt_kg_per_m2']) |
              (train['reanalysis_precip_amt_kg_per_m2'] > upper_bound['reanalysis_precip_amt_kg_per_m2'])]

# Mostrar el número de outliers y algunos ejemplos
print(f"Total de outliers en 'station_precip_mm': {(train['station_precip_mm'] > upper_bound['station_precip_mm']).sum()}")
print(f"Total de outliers en 'reanalysis_precip_amt_kg_per_m2': {(train['reanalysis_precip_amt_kg_per_m2'] > upper_bound['reanalysis_precip_amt_kg_per_m2']).sum()}")
outliers

# Exportar los outliers a un archivo CSV para ver mejor su contenido
outliers.to_csv('outliers_precipitation.csv', index=False)

"""**Observaciones Generales**
Número de Registros: Hay 158 registros en total que tienen al menos uno de los dos valores de precipitación considerados como outliers.

**Rango de Valores:**

station_precip_mm tiene valores que van desde 8.74 hasta 543.3. El valor máximo es bastante extremo en comparación con la mediana (123.6), lo que indica una amplia dispersión.
reanalysis_precip_amt_kg_per_m2 varía de 8.7 hasta 570.5, también con una gran dispersión en los valores, con una mediana de 115.49.

**Condiciones Climáticas Altamente Variables:** Los valores de humedad relativa y temperatura no presentan variaciones extremas, pero las precipitaciones sí, lo que sugiere eventos climáticos inusuales (como lluvias torrenciales).

Dado que las precipitaciones extremas pueden ser relevantes, mantendremos los outliers y aplicaremos una transformación logarítmica para reducir su impacto. Esta transformación permitirá que los valores altos no dominen el análisis de clustering pero aún conservará la información sobre eventos climáticos extremos.
"""

import numpy as np # Import the numpy library and assign it to the alias 'np'

# Aplicar la transformación logarítmica a las variables de precipitación
train['station_precip_mm_log'] = np.log1p(train['station_precip_mm'])
train['reanalysis_precip_amt_kg_per_m2_log'] = np.log1p(train['reanalysis_precip_amt_kg_per_m2'])

# Verificación de la transformación: Mostrar las primeras filas y comparar antes y después
train[['station_precip_mm', 'station_precip_mm_log', 'reanalysis_precip_amt_kg_per_m2', 'reanalysis_precip_amt_kg_per_m2_log']].head()

"""## **4. Imputación de valores nulos**

**Visualizamos la cantidad de nulos por columna:**
"""

null_counts = train.isnull().sum()
null_percent = (train.isnull().mean() * 100).round(2)
null_summary = pd.DataFrame({'Total Nulos': null_counts, 'Porcentaje Nulos (%)': null_percent})
print("Resumen de valores nulos en el dataset:")
print(null_summary[null_summary['Total Nulos'] > 0])

"""### Visualización de Nulos con un Mapa de Calor"""

plt.figure(figsize=(12, 8))
sns.heatmap(train.isnull(), cbar=False, cmap="viridis")
plt.title("Mapa de Calor de Nulos en el Dataset")
plt.show()

"""Viendo el mapa de calor de valores nulos, puedo notar algunos patrones interesantes. Las líneas amarillas representan los valores faltantes en el dataset, y es claro que no están distribuidos de forma uniforme. Algunas columnas tienen muchos nulos, mientras que otras están prácticamente completas.

**Observaciones principales:**

**Variables con muchos nulos:** Hay ciertas variables que muestran varios valores faltantes en distintas partes del dataset, especialmente en los índices de vegetación (ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw). Esto podría indicar que la información de vegetación no siempre estuvo disponible en todas las semanas o áreas.

**Patrones en los nulos:** Los nulos parecen estar concentrados en ciertos periodos o grupos, en lugar de estar distribuidos aleatoriamente. Esto sugiere que hay semanas o bloques de tiempo donde la recopilación de datos fue incompleta o afectada por alguna condición.

**Variación entre columnas:** Algunas variables tienen muy pocos o ningún valor nulo, lo cual es positivo, porque puedo contar con esa información para realizar análisis más completos.

Este mapa de calor me ayuda a visualizar de forma rápida dónde están los puntos débiles en mi dataset en cuanto a información faltante. Con esta visión, podré decidir si necesito eliminar ciertas filas, imputar valores en algunas columnas o trabajar con variables específicas para que el análisis sea más robusto.

### Análisis de Patrones de Nulos en Filas
"""

# Contar filas con diferentes cantidades de nulos
row_null_counts = train.isnull().sum(axis=1).value_counts()
print("\nDistribución de la cantidad de nulos por fila:")
print(row_null_counts)

"""Al observar la distribución de la cantidad de nulos por fila, puedo extraer algunas conclusiones sobre la estructura de los datos y cómo proceder:

**Observaciones Clave**

**Mayoría de filas sin nulos:** La mayoría de las filas (1,199) no tienen valores nulos, lo cual es positivo y significa que tenemos una buena cantidad de datos completos.

**Filas con pocos nulos:** Luego, tenemos grupos de filas con solo unos pocos nulos (168 filas con 1 nulo, 47 con 2, y así sucesivamente). Esto sugiere que muchos de estos nulos podrían ser manejables mediante imputación, ya que son pocos en relación al número total de filas.

**Filas con muchos nulos:** Algunas filas, aunque pocas (5 filas con 18 nulos y 5 con 22 nulos), presentan una cantidad significativa de valores faltantes. Estas filas tienen tantas variables nulas que podría ser difícil o poco confiable imputarlas.

###Correlación de nulos entre columnas
"""

# Crear un DataFrame de valores nulos (True/False) para calcular correlación
null_corr = train.isnull().corr()
plt.figure(figsize=(10, 8))
sns.heatmap(null_corr, annot=True, cmap="coolwarm", cbar=True)
plt.title("Correlación de Nulos entre Columnas")
plt.show()

"""Al revisar la correlación de nulos entre columnas, me doy cuenta de algunos patrones interesantes. Hay grupos de variables que tienden a tener valores faltantes al mismo tiempo, lo que me da pistas sobre cómo se estructuran los datos y me ayuda a planear cómo manejarlos.

**Lo que observé:**

**Variables de vegetación:** Las variables de vegetación (ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw) muestran una alta correlación en sus nulos. Esto significa que cuando falta un dato de vegetación en una de estas columnas, es muy probable que falten los datos en las demás también. Puede que en ciertos periodos o zonas no se haya recolectado esta información de vegetación de forma completa.

**Variables climáticas de reanálisis:** También noto un grupo de variables relacionadas con el clima (reanalysis_air_temp_k, reanalysis_avg_temp_k, reanalysis_dew_point_temp_k, etc.) que siguen un patrón similar. Cuando falta información en una de estas variables, suele faltar en varias otras del mismo tipo. Esto me indica que en ciertas fechas o lugares, los datos climáticos no se capturaron completamente.

**Pocas variables sin nulos:** Hay algunas columnas, como year, weekofyear y city, que prácticamente no tienen nulos. Esto es útil porque puedo contar con estas variables como referencia para entender cómo se distribuyen los datos y me podrían servir en la imputación de los valores faltantes.

### **Eliminar filas duplicadas**
"""

print(f"Filas duplicadas: {train.duplicated().sum()}")

train.drop_duplicates(inplace=True)

print(f"Filas duplicadas: {train.duplicated().sum()}")

"""### **Completar los valores faltantes con interpolación**"""

# Revisar el resumen de valores nulos después de la interpolación
remaining_nulls = train.isnull().sum()
remaining_nulls = remaining_nulls[remaining_nulls > 0]
print("Columnas con valores nulos restantes:")
print(remaining_nulls)

from sklearn.impute import SimpleImputer

# Imputación de valores nulos por media
print(f"\nValores nulos antes de la imputación:")
print(train.isnull().sum())

imputer = SimpleImputer(strategy='mean')
numeric_cols = train.select_dtypes(include=[np.number]).columns
train[numeric_cols] = imputer.fit_transform(train[numeric_cols])

print(f"\nValores nulos restantes: {train.isnull().sum()}")

# Calcular el total de valores nulos en el dataset
total_nulos = train.isnull().sum().sum()
print("\nTotal de valores nulos en el dataset:", total_nulos)

"""Binarización de la característica city"""

# Binarizador para la variable categórica city en el conjunto de entrenamiento y test
from sklearn import preprocessing
lb = preprocessing.LabelBinarizer()
train['city_bin'] = lb.fit_transform(train['city'])

train.tail()

"""**Eliminar las columnas que no son requeridas para este estudio**"""

# Lista de columnas relacionadas con la fecha
date_columns = ['year', 'weekofyear', 'city_bin', 'city','week_start_date']

# Eliminar las columnas de fecha del DataFrame
train.drop(columns=date_columns, inplace=True)

# Confirmar que se han eliminado las columnas
print("Columnas restantes en el dataset:")
print(train.columns)

"""**Histogramas**"""

# Visualización de la distribución de variables numéricas
import matplotlib.pyplot as plt
train.hist(bins=20, figsize=(20,15))
plt.show()

"""En esta sección, analizo la distribución de mis variables antes de proceder con los métodos de clustering. Al observar estos histogramas, se pueden ver algunas características interesantes en los datos:

Distribución Simétrica: Algunas variables, como ndvi_ne, ndvi_nw, ndvi_se y ndvi_sw, tienen una distribución aproximadamente simétrica, similar a una curva normal. Esto indica que los valores están concentrados en torno a una media central, lo cual es útil para ciertos tipos de análisis y puede facilitar la interpretación de patrones.

Distribución Sesgada: Otras variables, como precipitation_amt_mm, station_diur_temp_rng_c y station_precip_mm, muestran una clara sesgo positivo (hacia la derecha). Esto sugiere que la mayoría de los valores se encuentran en un rango bajo, pero existen valores más altos que podrían ser considerados outliers o puntos extremos. Este tipo de sesgo es común en datos de precipitación y temperaturas extremas.

Variables con Rangos Reducidos: Algunas variables, como reanalysis_avg_temp_k y station_min_temp_c, presentan un rango de valores relativamente pequeño, lo cual puede influir en la escala y sensibilidad de los modelos de clustering. Es probable que estas variables necesiten ser estandarizadas para asegurar que todas las variables tengan un peso equivalente en el análisis.

Variables Dispersas y Concentradas: Variables como station_precip_mm_log tienen una dispersión de valores notablemente más amplia comparada con otras, lo cual indica variabilidad significativa en los datos. Por otro lado, variables como reanalysis_relative_humidity_percent están concentradas en un rango más estrecho.

Características Diferentes en la Humedad y Temperatura: Las variables relacionadas con la humedad, como reanalysis_specific_humidity_g_per_kg y reanalysis_relative_humidity_percent, muestran diferentes patrones de distribución. La primera tiene una distribución más ancha y sesgada, mientras que la segunda es más concentrada y uniforme, lo cual puede indicar que estos factores influyen de maneras distintas en el análisis de datos meteorológicos.

Este análisis preliminar de la distribución me ayudará a ajustar los parámetros de los métodos de clustering y decidir sobre transformaciones adicionales, como la estandarización, que puedan mejorar la calidad de los agrupamientos.

---
# Características
---

En esta sección, llevaremos a cabo un análisis exhaustivo de selección de características, evaluando tanto el **dataset traspuesto** como el **dataset original** para una comprensión completa de las variables. Esto significa que no solo analizaremos las características por sí mismas, sino que también consideraremos la aplicación de técnicas de **clustering y detección de outliers** en ambas configuraciones para asegurar que obtenemos un conjunto óptimo de características.

Resultado Final de la Selección de Características
El objetivo al final de esta sección será obtener un **nuevo conjunto de características** que podrá incluir:
- **Características originales** sin modificar.
- **Características originales transformadas**, si es necesario aplicar una transformación que mejore su relevancia o distribuya mejor sus valores.
- **Características nuevas derivadas** de la combinación de las características originales.

Aunque en los experimentos es posible trabajar con todas las características, el propósito aquí es demostrar la capacidad de identificar correlaciones y seleccionar únicamente las características más relevantes, eliminando aquellas redundantes para optimizar el rendimiento de los algoritmos de machine learning.

Consideraciones sobre la Normalización
En aprendizaje no supervisado, la **normalización** de los datos suele ser beneficiosa, ya que ayuda a equilibrar la influencia de cada característica en los algoritmos de clustering y detección de patrones. Sin embargo, la conveniencia de normalizar dependerá del **problema específico** y la naturaleza de las características meteorológicas. En algunos casos, la normalización puede no ser necesaria o incluso perjudicial si afecta la interpretación de los datos o introduce distorsiones en la escala de las variables.

Para este análisis, nos enfocaremos exclusivamente en las **variables meteorológicas** y no incluiremos ninguna variable relacionada con **nuevos contagios**.

Utilizamos la librería Pandas con el fin de poder manejar los datos en una estructura denominada DataFrame. Además, con el objetivo de analizar las características con este estudio de aprendizaje no supervisado, vamos a transponer el dataset, de modo que las características originales serán ahora los nuevos "elementos" y los elementos originales las nuevas "características".
"""

import matplotlib.pyplot as plt
import numpy as np
import itertools
import seaborn as sns
from numpy import corrcoef, transpose, arange
from pylab import pcolor, show, colorbar, xticks, yticks

import pandas as pd
import io
df_feat = transpose(train)
df_feat

"""Estos datos contienen para cada uno de los estados (State) las diferentes cantidades en producto interior bruto dentro de categorías que denominaríamos de nivel 2 de agregación, es decir, son una desagragción de niveles superiores. De esta forma encontramos datos como el PIB de Agricultura, Minería, Construcción, Comercio, etc.

Una vez que contamos con el DataFrame de Pandas podríamos analizar qué características necesitamos y cuales deberíamos excluir.
"""

names =  df_feat.index
names

"""# Correlación entre variables

Con el fin de que obtener un mejor rendimiento en el algoritmo de clustering será necesario identificar aquellas variables que son redundantes, es decir, se puede asumir que representan lo mismo, en este caso se puede utilizar el análisis de correlaciones.
"""

# Convert all columns in df_feat to numeric, replacing errors with NaN
df_feat = df_feat.apply(pd.to_numeric, errors='coerce')

# Drop columns with all NaN values
df_feat = df_feat.dropna(axis=1, how='all')

# Calculate correlation matrix
R = np.corrcoef(df_feat, rowvar=False)  # rowvar=False to calculate correlation between columns

#https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html
# R = corrcoef(df_feat)

# http://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html
# Generate a mask for the upper triangle
sns.set(style="white")
mask = np.zeros_like(R, dtype=bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(200, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(R, mask=mask, cmap=cmap, vmax=.8,
            square=True, xticklabels=names, yticklabels=names,
            linewidths=.5, cbar_kws={"shrink": .5}, ax=ax)

"""Al generar el primer gráfico de correlación, me encontré con un problema en la visualización: el gráfico se veía comprimido, y los nombres de las variables no eran legibles. Esto dificultaba bastante la interpretación, ya que no podía identificar claramente las correlaciones entre las variables.

Para mejorar la visualización, decidí hacer algunos ajustes. Primero, aumenté el tamaño de la figura para que las etiquetas de las variables fueran más visibles. También apliqué una máscara para ocultar la mitad superior de la matriz, lo que eliminó la duplicación de información y facilitó la lectura. Además, utilicé un esquema de colores más claro, que resalta mejor los valores altos y bajos de correlación, y añadí anotaciones para mostrar los valores exactos dentro de cada celda.

Gracias a estos cambios, el gráfico ahora es mucho más fácil de interpretar. Puedo ver claramente qué variables tienen una correlación alta y cuáles no, lo que me ayudará en la selección de características y en la optimización del modelo.
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Calcular la matriz de correlación usando pandas
correlation_matrix = train.corr()

# Configurar el estilo de seaborn y crear la máscara para la mitad superior
sns.set(style="white")
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Configurar el tamaño de la figura
plt.figure(figsize=(11, 9))

# Generar un mapa de colores divergente (de rojo a azul)
cmap = sns.diverging_palette(200, 10, as_cmap=True)

# Dibujar el heatmap con la máscara y la relación de aspecto correcta
sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, vmin=-1, vmax=1,
            square=True, annot=True, fmt=".2f",
            linewidths=0.5, cbar_kws={"shrink": 0.5})

plt.title("Matriz de Correlación entre Variables Meteorológicas")
plt.show()

"""Al observar el gráfico de correlación actualizado, puedo identificar varias relaciones interesantes entre las variables meteorológicas. Aquí destaco algunas de las correlaciones más notables:

1. **Variables de Temperatura**:
   - Las variables de temperatura, como `station_max_temp_c` y `reanalysis_max_air_temp_k`, están fuertemente correlacionadas entre sí, con un valor de correlación cercano a 0.70. Esto sugiere que ambas variables reflejan una información similar, y podría considerar mantener solo una de ellas para reducir redundancia en el análisis.
   - De manera similar, `station_min_temp_c` y `reanalysis_min_air_temp_k` también muestran una alta correlación, lo cual indica que estas variables también pueden ser redundantes.

2. **Variables de Humedad**:
   - Veo una correlación fuerte entre `reanalysis_specific_humidity_g_per_kg` y `reanalysis_relative_humidity_percent`, con un valor alrededor de 0.55. Ambas variables están relacionadas con la humedad en el ambiente, y podría seleccionar solo una de ellas si quiero simplificar el modelo sin perder mucha información relevante.
   - Además, `station_avg_temp_c` y `station_diur_temp_rng_c` también presentan una correlación con estas variables de humedad, lo cual puede estar reflejando condiciones ambientales similares.

3. **Variables de Precipitación**:
   - `station_precip_mm` y `reanalysis_precip_amt_kg_per_m2` tienen una correlación significativa (alrededor de 0.80), indicando que capturan patrones similares de precipitación en el dataset. Esto sugiere que, al seleccionar variables, podría mantener una sola de ellas.
   - La variable `precipitation_amt_mm` muestra correlaciones moderadas con otras variables de precipitación y temperatura, indicando que podría aportar una perspectiva ligeramente diferente, pero similar en general.

4. **Índices de Vegetación**:
   - Las variables relacionadas con el índice de vegetación, como `ndvi_ne` y `ndvi_nw`, tienen una correlación de alrededor de 0.6. Esto indica que estas variables están capturando patrones similares de vegetación y, por lo tanto, podría considerar eliminar una para reducir redundancia.
   - Sin embargo, estas variables también están moderadamente correlacionadas con algunas de las variables de temperatura y precipitación, lo que podría reflejar el impacto del clima en la vegetación.

### Conclusión
Este análisis de correlación me ha permitido identificar varias variables que están altamente correlacionadas entre sí, lo que me lleva a considerar una reducción en el número de características. Podría seleccionar una de cada par de variables altamente correlacionadas (por ejemplo, entre variables de temperatura y precipitación) para simplificar el modelo sin perder mucha información. Este paso ayudará a optimizar los algoritmos de machine learning, evitando que variables redundantes afecten el rendimiento del análisis.

# Análisis de componentes principales

Con el fin de observar como están relacionadas las variables también podemos hacer una representación en PCA de las mismas
"""

# Import PCA from sklearn.decomposition
from sklearn.decomposition import PCA

# Calcular la varianza acumulada para cada número de componentes
pca = PCA().fit(train)
explained_variance = pca.explained_variance_ratio_.cumsum()

# Graficar la varianza explicada acumulada
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')
plt.xlabel('Número de Componentes')
plt.ylabel('Varianza Explicada Acumulada')
plt.title('Varianza Explicada por los Componentes Principales')
plt.grid(True)
plt.show()

"""Al analizar la gráfica de varianza explicada acumulada, me doy cuenta de que la curva se estabiliza alrededor de los primeros 3 o 4 componentes. Esto significa que, a partir del quinto componente, la cantidad de varianza adicional capturada por cada nuevo componente es mínima. En otras palabras, los primeros 5 componentes ya logran captar casi toda la información relevante del dataset, alcanzando aproximadamente el 95% de la varianza total.

Con base en esto, he decidido establecer el valor de n_components en 4 para el análisis de PCA. Esto me permitirá reducir las dimensiones del dataset a solo 4 componentes, lo cual simplificará el análisis sin perder detalles importantes. Este ajuste me ayuda a mantener un buen balance entre la reducción de dimensionalidad y la retención de información, haciendo que los datos sean más manejables y eficientes para los siguientes pasos del análisis.

Tras hacer un análisis al probar con 4 o 5 componentes, he decidido quedarme con 20.
"""

#1. Normalization of the data
#http://scikit-learn.org/stable/modules/preprocessing.html
from sklearn import preprocessing
from sklearn.impute import SimpleImputer

min_max_scaler = preprocessing.MinMaxScaler()

# Impute NaN values with the mean of each column before scaling
imputer = SimpleImputer(strategy='mean') # You can choose a different strategy if needed
features_imputed = imputer.fit_transform(df_feat)

features_norm = min_max_scaler.fit_transform(features_imputed)

#1.2. Principal Component Analysis
from sklearn.decomposition import PCA
estimator = PCA (n_components = 20)
X_pca = estimator.fit_transform(features_norm)
print("Componentes lineales:\n", estimator.components_)
print("\nRatio de variabilidad: ", estimator.explained_variance_ratio_, "\n")

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
for i in range(len(X_pca)):
    plt.text(X_pca[i][0], X_pca[i][1], names[i])

plt.xlim(-2, 6)
plt.ylim(-1, 1.5)
ax.grid(True)
fig.tight_layout()
plt.show()

"""# Clustering jerárquico

Dado que tenemos pocos elementos que estudiar podemos utilizar clustering jerárquico para observar las relaciones de similitud entre el desarrollo de las variables. Aquellas grupos de variables que sean similares entre ellas pueden ser resumidas escogiendo una de ellas y de esa forma reduciríamos la dimensionalidad del conjunto de datos.

Para evitar la maldición de la dimensionalidad se puede hacer el clustering con los resultados del análisis de componentes principales, aunque hay que ser precavido ya que aunque reducida, existe perdida de variabilidad en los datos cuando se hace la proyección.
"""

# 2. Compute the similarity matrix
#http://docs.scipy.org/doc/scipy/reference/cluster.html
from scipy import cluster
import sklearn.metrics
dist = sklearn.metrics.DistanceMetric.get_metric('euclidean')
matdist= dist.pairwise(features_norm)

# 3.1.1 Visualization
import seaborn as sns; sns.set()
ax = sns.heatmap(matdist,vmin=0, vmax=1, yticklabels = names, xticklabels = names)

"""Este gráfico representa la **matriz de correlación** de las variables de mi dataset, lo cual permite identificar las relaciones entre ellas. En el mapa de calor, los valores de correlación están codificados en colores: los tonos oscuros representan valores cercanos a cero (poca o nula correlación), mientras que los tonos más claros representan valores de correlación más fuertes, ya sea positiva o negativa.

### Análisis

1. **Correlaciones altas**:
   - Existen bloques de correlación clara entre algunas variables, lo cual indica relaciones lineales fuertes entre ellas. Por ejemplo, el grupo de variables en la esquina superior izquierda tiene una correlación moderada a alta. Esto es común cuando las variables representan mediciones relacionadas de alguna manera, como condiciones climáticas medidas en diferentes estaciones o regiones.
   - Las variables relacionadas con la temperatura (`station_avg_temp_c`, `station_min_temp_c`, y `station_max_temp_c`) muestran una correlación fuerte entre sí, lo cual es esperado, dado que son medidas de una misma variable subyacente en diferentes contextos (mínimo, máximo, promedio).

2. **Poca correlación entre otras variables**:
   - Algunas variables no tienen correlación fuerte con otras, mostrando valores de correlación cercanos a cero (colores oscuros en el gráfico). Esto puede indicar que estas variables aportan información independiente y podrían ser útiles para análisis más complejos o para algoritmos de clustering.

3. **Potenciales redundancias**:
   - Las variables altamente correlacionadas pueden implicar redundancia. Esto es útil a la hora de realizar técnicas de reducción de dimensionalidad como PCA, ya que aquellas variables con alta correlación podrían ser combinadas sin perder mucha información.

En resumen, este análisis de la matriz de correlación sugiere que algunas variables están interrelacionadas y pueden capturar información redundante. Esto es valioso al tomar decisiones sobre selección de variables o simplificación del modelo.
"""

# 3. Building the Dendrogram
# http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage
clusters = cluster.hierarchy.linkage(matdist, method = 'single')
# http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html
cluster.hierarchy.dendrogram(clusters, color_threshold = 3, labels = names , leaf_rotation=90)
plt.show()

"""Vamos a obtener la asignación de grupos de cada uno de ellos"""

cut = 3 # !!!! ad-hoc
labels = cluster.hierarchy.fcluster(clusters, cut , criterion = 'distance')

labels

import matplotlib.pyplot as plt

# Ajustar el tamaño de la figura
plt.figure(figsize=(16, 12))

# Graficar los puntos originales con colores
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=70, cmap="rainbow", alpha=0.8)

# Añadir etiquetas para cada punto, con rotación
for i, name in enumerate(names):
    plt.text(X_pca[i, 0], X_pca[i, 1], name, fontsize=10, alpha=0.85, rotation=45)  # Rotar las etiquetas 45 grados

# Etiquetas de los ejes y título
plt.xlabel('Componente Principal 1', fontsize=12)
plt.ylabel('Componente Principal 2', fontsize=12)
plt.title('Visualización PCA de Variables', fontsize=14)

# Mostrar la cuadrícula
plt.grid(True)
plt.show()

"""Este gráfico muestra la visualización de las variables en el espacio reducido después de aplicar el Análisis de Componentes Principales (PCA), donde solo se utilizan los dos primeros componentes principales. Cada punto representa una variable del dataset original, y la posición en el gráfico indica la contribución de cada variable a estos componentes. Las variables que están más alejadas del origen, como station_precip_mm, reanalysis_precip_amt_kg_per_m2, y reanalysis_relative_humidity_percent, tienen una mayor influencia en los primeros componentes, lo cual sugiere que estas variables capturan más de la variabilidad explicada en el dataset. Las agrupaciones de puntos indican que algunas variables tienen contribuciones similares en el espacio de componentes principales, lo cual puede ser útil para identificar relaciones o redundancias entre variables.

# DBSCAN

Vamos a identificar elementos outliers mediante la utilización de DBSCAN. El propósito de identificar ouliter es aislar aquellas características que son diferentes a los demás y que nos pueden ayudar a distinguir a los diferentes elementos a los que representan.

### Parametrización

Vamos a fijar MinPts = 2 (en 3 genera uno o ningún cluster), y observar qué epsilon podemos establecer
"""

minPts=2
from sklearn.neighbors import kneighbors_graph
A = kneighbors_graph(features_norm, minPts, include_self=False)
Ar = A.toarray()

seq = []
for i,s in enumerate(features_norm):
    for j in range(len(features_norm)):
        if Ar[i][j] != 0:
            seq.append(matdist[i][j])

seq.sort()
# establecer intervalo ejes
fig = plt.figure()
ax = fig.gca()
ax.set_xticks(np.arange(0, 120, 10))
ax.set_yticks(np.arange(0, 3, 0.2))

plt.plot(seq)


plt.show()

"""Vamos a establecer un rango amplio de pruebas teniendo en cuenta que buscamos aislar elementos diferentes a los demás.

## Ejecución
"""

from sklearn.cluster import DBSCAN

for eps in np.arange(0.50, 1.8, 0.20):
  db = DBSCAN(eps, min_samples=minPts).fit(features_norm)
  core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
  core_samples_mask[db.core_sample_indices_] = True
  labels = db.labels_
  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
  n_outliers = list(labels).count(-1)
  print ("%6.2f, %d, %d" % (eps, n_clusters_, n_outliers))

#labels

"""Como podemos observar DBSCAN nos agrupa los elementos en un grupo y el resto son outliers. El número de outliers si es significativo tenerlo en cuenta, por lo que nos vamos a quedar con las parametrizaciones que nos ofrecen un mayor número de outliers."""

db = DBSCAN(eps=0.7, min_samples=minPts).fit(features_norm)
labels = db.labels_
labels

"""Visualizamos los resultados"""

#plotting orginal points with color related to label
plt.scatter(X_pca[:,0], X_pca[:,1], c=labels,s=50, cmap="rainbow")
for i in range(len(X_pca)):
    plt.text(X_pca[i][0], X_pca[i][1], names[i][0:3])
plt.grid()
plt.show()

"""Y aunque podríamos proceder a su intepretación en este caso lo que vamos a hacer es identificar aquellos elementos que son outliers"""

df_feat['dbscan_group'] = labels
df_feat[df_feat['dbscan_group'] == -1]

"""## **Creación de nuevas características**"""

# Rango térmico: diferencia entre temperatura máxima y mínima
train['temperature_range_c'] = train['station_max_temp_c'] - train['station_min_temp_c']

# Promedio de NDVI
train['ndvi_avg'] = train[['ndvi_ne', 'ndvi_se', 'ndvi_sw']].mean(axis=1)

"""## **Normalización**

Estas serían las características que utilizaría para mi estudio.
"""

# Normalización
# features_to_scale = ['reanalysis_precip_amt_kg_per_m2', 'reanalysis_relative_humidity_percent',
#                      'station_precip_mm', 'precipitation_amt_mm', 'temperature_range_c', 'ndvi_avg']

"""Para este estudio he decidido aplicar todas las características que tengo hasta este punto."""

# Normalización con MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(train)
X_train

"""## **Trasponer el dataset**"""

# Transponer el dataset
train_transposed = train.T

# Ver las primeras filas del dataset traspuesto
print(train_transposed.head())

"""Al transponer el dataset, hemos intercambiado las filas y las columnas, lo que significa que ahora cada característica original es una fila, y cada observación temporal (instancia) es una columna. Esto nos permite visualizar y analizar las características de forma individual y facilita la comparación de valores a lo largo del tiempo o entre observaciones.

Esta estructura puede ser especialmente útil para exploraciones detalladas de cada variable en términos de tendencia y estacionalidad, y también es conveniente para análisis que involucren estadísticas descriptivas o visualización de series temporales. Por ejemplo, podemos observar el comportamiento de la vegetación (ndvi_ne, ndvi_se, etc.) o los cambios de temperatura y precipitación por año y semana de forma más clara y alineada, facilitando cualquier inspección específica.
"""

# Matriz de correlación del dataset traspuesto
transposed_correlation_matrix = train_transposed.corr()

# Mostrar la matriz de correlación del dataset traspuesto
print(transposed_correlation_matrix)

# Crear un gráfico de calor (heatmap) de la matriz de correlación
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title('Correlation Matrix of Features')
plt.show()

"""Para realizar el clustering, he decidido trabajar con el dataset sin transponerlo, ya que esto facilita agrupar las observaciones (semanas) en función de sus características, como temperatura, humedad y precipitación. Al mantener el formato original, cada fila representa una semana completa con todas sus variables, lo cual es ideal para identificar patrones y similitudes entre semanas. Aunque la transposición es útil para ciertos análisis exploratorios, en el clustering quiero evaluar cada observación como un conjunto de atributos, lo que permitirá crear clusters más representativos y coherentes.

---
# Clustering
---

En esta sección realizaré un análisis de clustering utilizando diversas técnicas para determinar cuál de ellas se adapta mejor a los datos de este dataset. El objetivo es identificar y comparar la calidad del agrupamiento generado por cada método, evaluando tanto la cohesión como la separación de los clusters. Para esto, aplicaré al menos dos técnicas de clustering, considerando también la posibilidad de explorar algoritmos adicionales que no se hayan tratado en clase para obtener resultados más completos y variados.

Dado que el dataset cuenta con múltiples dimensiones, utilizaré técnicas de reducción de dimensionalidad, como el Análisis de Componentes Principales (PCA), para proyectar los datos en 2D o 3D y facilitar la visualización de los clusters. Sin embargo, los análisis y conclusiones se basarán en las dimensiones originales para capturar de forma precisa la estructura de los datos.

Además de ejecutar los algoritmos, interpretaré los resultados obtenidos de cada técnica, comparándolos en base a métricas específicas y deduciendo qué factores hacen que un método funcione mejor que otro. A través de esta comparación exhaustiva, espero no solo identificar el algoritmo de clustering más adecuado, sino también entender cómo cada técnica responde a las características particulares de este dataset.

##Clustering Jerárquico

###Dendrograma Ward
"""

from scipy.cluster.hierarchy import dendrogram, linkage

# Generar el dendrograma
linked = linkage(X_train, method='ward')

plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.title('Dendrograma de Clustering Jerárquico')
plt.xlabel('Instancias')
plt.ylabel('Distancia Euclídea')
plt.show()

"""En este dendograma no podemos visualizar bien las instancias por lo que procedemos a truncarlo"""

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Crear la matriz de enlaces (linkage matrix) usando el método 'ward' o cualquier otro método
Z = linkage(X_train, method='ward')

# Graficar el dendrograma truncado
plt.figure(figsize=(10, 7))
plt.title('Hierarchical Clustering Dendrograma (truncado)')
plt.xlabel('Índice de la muestra')
plt.ylabel('Distancia')
dendrogram(
    Z,
    truncate_mode='lastp',  # mostrar solo las últimas p uniones
    p=12,  # definimos p (número de últimas uniones a mostrar)
    leaf_rotation=90.,  # rotar los nombres de las hojas
    leaf_font_size=12.,  # tamaño de la fuente de las hojas
    show_contracted=True,  # mostrar ramas truncadas
)
plt.show()

"""En este dendograma si cortamos por el número 8 podemos diferenciar claramente los 3 cluster."""

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Crear la matriz de enlaces (linkage matrix) usando el método 'ward'
Z = linkage(X_train, method='ward')

# Definir la función "fancy" para el dendrograma
def fancy_dendrogram(*args, **kwargs):
    max_d = kwargs.pop('max_d', None)
    annotate_above = kwargs.pop('annotate_above', 0)
    ddata = dendrogram(*args, **kwargs)

    if not kwargs.get('no_plot', False):
        plt.title('Dendrograma con línea de corte')
        plt.xlabel('Índice de la muestra')
        plt.ylabel('Distancia')
        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):
            x = 0.5 * sum(i[1:3])
            y = d[1]
            if y > annotate_above:
                plt.plot(x, y, 'ro')
                plt.annotate(f'{y:.3f}', (x, y), xytext=(0, -5),
                             textcoords='offset points', va='top', ha='center')
        if max_d:
            plt.axhline(y=max_d, c='k')
    return ddata

# Parámetros del dendrograma
d_max = 8  # Distancia máxima para el corte

# Graficar el dendrograma con la línea de corte
plt.figure(figsize=(10, 7))
fancy_dendrogram(
    Z,
    truncate_mode='lastp',
    p=12,
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True,
    annotate_above=1,  # Anotar por encima de este valor
    max_d=d_max,  # Añadir la línea de corte horizontal
)
plt.show()

"""###Aplicar Clustering jerárquico con el método ward"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import pandas as pd

# Aplicar clustering jerárquico con 3 clusters
hierarchical = AgglomerativeClustering(n_clusters=3, linkage='ward')
hierarchical_clusters = hierarchical.fit_predict(X_train)

# Calcular el índice de silueta
silhouette_hierarchical = silhouette_score(X_train, hierarchical_clusters)

print(f"Índice de silueta para Clustering Jerárquico con el método ward: {silhouette_hierarchical}")

"""Un índice de silueta de 0.291 para el clustering jerárquico indica una calidad de agrupamiento relativamente baja. Este valor, que se encuentra por debajo de 0.3, sugiere que los clusters tienen una separación y cohesión moderadas, lo cual puede indicar que los puntos de datos no están completamente bien agrupados dentro de sus respectivos clusters. En este caso, algunos puntos pueden estar ubicados cerca de los límites de los clusters o incluso más próximos a otros clusters que al suyo propio.

###Reducción de dimensionalidad a 3D usando PCA
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Necesario para gráficos en 3D

# Reducir las dimensiones a 3 usando PCA
pca_3d = PCA(n_components=3)
train_pca_3d = pca_3d.fit_transform(X_train)

# Graficar los clusters jerárquicos en 3D
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(train_pca_3d[:, 0], train_pca_3d[:, 1], train_pca_3d[:, 2], c=hierarchical_clusters, cmap='viridis')
plt.colorbar(sc)
ax.set_title("Clusters Jerárquicos (3D PCA)")
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
plt.show()

"""###Reducción de dimensionalidad a 2D usando PCA"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reducir las dimensiones a 2 usando PCA
pca_2d = PCA(n_components=2)
train_pca_2d = pca_2d.fit_transform(X_train)

# Graficar los clusters jerárquicos en 2D
plt.figure(figsize=(10, 7))
plt.scatter(train_pca_2d[:, 0], train_pca_2d[:, 1], c=hierarchical_clusters, cmap='viridis')
plt.title("Clusters Jerárquicos (2D PCA)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()

"""En el gráfico de clusters jerárquicos tras aplicar PCA, observamos que los puntos se agrupan en tres clusters distintos. Sin embargo, estos clusters presentan cierta superposición, lo cual concuerda con el bajo índice de silueta de 0.016 obtenido previamente. La separación entre los clusters no es tan clara como se desearía, lo que indica que los grupos formados pueden no ser óptimos. Los clusters no están bien definidos, y los datos dentro de cada cluster no son lo suficientemente compactos. Este resultado sugiere que el método de clustering jerárquico podría no ser el más adecuado para capturar la estructura de este dataset, y que podría ser conveniente explorar otros métodos de clustering para obtener una separación más nítida de los datos.

###Dendrograma con el método complete

Dendrograma truncado para el método 'complete'
"""

# Crear la matriz de enlaces (linkage matrix) usando el método 'complete'
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

Z_complete = linkage(X_train, method='complete')

# Graficar el dendrograma truncado para el método 'complete'
plt.figure(figsize=(10, 7))
plt.title('Dendrograma de Clustering Jerárquico (complete, truncado)')
plt.xlabel('Índice de la muestra')
plt.ylabel('Distancia')
dendrogram(
    Z_complete,
    truncate_mode='lastp',
    p=12,
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True,
)
plt.show()

"""#### Graficar el dendrograma con línea de corte (complete)"""

plt.figure(figsize=(10, 7))
plt.title('Dendrograma con línea de corte (complete)')
plt.xlabel('Índice de la muestra')
plt.ylabel('Distancia')
dendrogram(
    Z_complete,
    truncate_mode='lastp',
    p=12,
    leaf_rotation=90.,
    leaf_font_size=12.,
    show_contracted=True
)
plt.axhline(y=2.1, c='k')  # Línea de corte en la distancia 550
plt.show()

"""En el dendograma podemos visualizar con facilidad que tenemos 3 clusters.

### Aplicar Clustering jerárquico con el método complete

Procedemos a hacer otro experimento pero cambiando el linkage a complete
"""

from sklearn.cluster import AgglomerativeClustering

# Cambiar el método de enlace a 'complete' o 'average'
hierarchical_complete = AgglomerativeClustering(n_clusters=3, linkage='complete')
clusters_complete = hierarchical_complete.fit_predict(X_train)

silhouette_complete = silhouette_score(X_train, clusters_complete)
print(f"Índice de silueta con linkage 'complete': {silhouette_complete}")

"""Un índice de silueta de aproximadamente 0.297 para el clustering jerárquico utilizando el método de enlace 'complete' indica una calidad de agrupamiento moderada. Aunque este valor está por debajo de 0.5, sugiere que los clusters tienen una cierta cohesión interna y separación entre ellos, aunque no son completamente bien definidos.

### Visualización 3D aplicando PCA metodo complete
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Reducir las dimensiones a 3 usando PCA
pca = PCA(n_components=3)
train_pca = pca.fit_transform(X_train)

# Visualización de los clusters jerárquicos en 3D
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(train_pca[:, 0], train_pca[:, 1], train_pca[:, 2], c=clusters_complete, cmap='viridis')
ax.set_title("Clusters Jerárquicos (3D PCA)")
ax.set_xlabel("PCA Component 1")
ax.set_ylabel("PCA Component 2")
ax.set_zlabel("PCA Component 3")
plt.show()

"""En este gráfico podemos visualizar que los clústers están bien definidos, ya que es notable la diferencia de colores, pero en algunas partes se superpone un poco.

###Visualización en 2D con PCA
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reducir las dimensiones a 2 usando PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(X_train)

# Graficar los clusters predichos
plt.figure(figsize=(10, 7))
plt.scatter(data_pca[:, 0], data_pca[:, 1], c=clusters_complete, cmap='viridis')
plt.title("Visualización de Clusters (PCA 2D)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()

"""En el análisis de clustering jerárquico, probé diferentes métodos de enlace para encontrar la configuración que mejor agrupara los datos. Usando el método de enlace completo (linkage 'complete'), obtuve un índice de silueta de 0.2968, ligeramente superior al índice de 0.2911 obtenido con el método por defecto. Estos valores, aunque cercanos, sugieren que el enlace completo logra un agrupamiento marginalmente mejor en cuanto a cohesión y separación de los clusters, aunque el índice sigue siendo bajo en general. Esto implica que los clusters formados no son altamente compactos ni bien diferenciados, indicando posibles limitaciones del clustering jerárquico para estos datos.

##K-Means
"""

# Importar las librerías necesarias
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Configurar el estilo de los gráficos
sns.set(style="whitegrid")

# Función para realizar K-means y calcular el índice de silueta
def kmeans_clustering(X, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(X)
    silhouette_avg = silhouette_score(X, labels)

    print(f"\nPara {n_clusters} clusters, el índice de silueta promedio es: {silhouette_avg:.2f}")

    # Visualización de los clusters
    plt.figure(figsize=(8, 6))
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=40)
    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='*', label='Centroides')
    plt.title(f"K-means con {n_clusters} Clusters")
    plt.xlabel("Componente Principal 1")
    plt.ylabel("Componente Principal 2")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Gráfico de silueta para cada cluster
    plt.figure(figsize=(8, 6))
    silhouette_values = silhouette_samples(X, labels)
    y_lower = 10
    for i in range(n_clusters):
        ith_cluster_silhouette_values = silhouette_values[labels == i]
        ith_cluster_silhouette_values.sort()
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
        plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, alpha=0.7)
        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10
    plt.axvline(x=silhouette_avg, color="red", linestyle="--")
    plt.title(f"Gráfico de Silueta para {n_clusters} Clusters")
    plt.xlabel("Coeficiente de Silueta")
    plt.ylabel("Cluster")
    plt.show()

# Aplicación de K-means con 2, 3 y 4 clusters
for n_clusters in [2, 3, 4]:
    kmeans_clustering(X_train, n_clusters)

"""En este análisis de clustering usando K-means, evalué diferentes configuraciones de clusters para encontrar la que mejor representara la estructura de los datos. Al configurar el modelo para 2 clusters, obtuve el índice de silueta más alto con un valor de 0.37, lo que indica que, en promedio, los puntos están mejor agrupados y más claramente separados de otros clusters en esta configuración. Al incrementar a 3 clusters y 4 clusters, el índice de silueta disminuyó a 0.32 y 0.31, respectivamente, lo que sugiere una menor cohesión y separación en estos agrupamientos. En general, el resultado sugiere que una configuración de 2 clusters es la más adecuada para este dataset, aunque el valor del índice indica una calidad de clustering moderada.

##DBSCAN

DBSCAN es un método de clustering basado en densidad que identifica áreas de alta densidad de puntos como clusters, lo que permite detectar ruido y agrupaciones de formas arbitrarias.

El siguiente código calcula la distancia promedio al vecino más cercano y grafica la curva K-dist. Utilizaré este gráfico para identificar la "rodilla" y seleccionar un valor adecuado para eps.
"""

from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# Calculamos las distancias al vecino más cercano
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(X_train)
distances, indices = neighbors_fit.kneighbors(X_train)

# Ordenamos las distancias y hacemos el gráfico
distances = np.sort(distances[:, 4], axis=0)  # usamos la distancia al quinto vecino más cercano
plt.plot(distances)
plt.xlabel('Puntos de datos ordenados')
plt.ylabel('Distancia al 5º vecino más cercano')
plt.title('Curva K-dist para selección de eps')
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# Experimenta con valores de eps basados en la curva K-dist
for eps in [0.3, 0.35, 0.4, 0.45, 0.5]:
    for min_samples in [5, 7, 10]:
        # Configuración y aplicación de DBSCAN
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        clusters = dbscan.fit_predict(X_train)

        # Calcular el número de clusters
        n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)

        if n_clusters > 1:  # Solo calcular índice de silueta si hay al menos 2 clusters
            silhouette_avg = silhouette_score(X_train, clusters)
            print(f"eps: {eps}, min_samples: {min_samples}, Índice de silueta: {silhouette_avg}, Clusters: {n_clusters}")
        else:
            print(f"eps: {eps}, min_samples: {min_samples}, Only {n_clusters} cluster(s) found. Cannot calculate silhouette score.")

from sklearn.decomposition import PCA

# Reducimos a 2 dimensiones para visualización
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train)

# Visualizamos los clusters generados por DBSCAN
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='jet', s=50, alpha=0.7)
plt.title("DBSCAN Clustering")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.colorbar(label="Cluster")
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
import numpy as np

# Ajustamos DBSCAN con los mejores parámetros
dbscan_best = DBSCAN(eps=0.45, min_samples=5)
clusters_best = dbscan_best.fit_predict(X_train)

# Calculamos el índice de silueta
silhouette_avg = silhouette_score(X_train, clusters_best)
print(f"Índice de silueta para DBSCAN (eps=0.45, min_samples=5): {silhouette_avg}")

# Graficamos los clusters
plt.figure(figsize=(10, 7))
plt.scatter(X_train[:, 0], X_train[:, 1], c=clusters_best, cmap='viridis', s=50, alpha=0.6)
plt.title("DBSCAN Clustering con eps=0.45 y min_samples=5")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Cluster")
plt.grid(True)
plt.show()

"""En el análisis con DBSCAN, observé cómo el parámetro de distancia máxima permitida entre puntos para ser considerados vecinos (eps) y el número mínimo de puntos requeridos para formar un cluster (min_samples) influyen en la cantidad y cohesión de los clusters generados.

Cuando probé con un eps de 0.3, el índice de silueta fue negativo o cercano a cero, especialmente con valores bajos de min_samples, lo que sugiere una falta de cohesión en los clusters y posibles problemas de ruido en los datos. A medida que incrementé eps a 0.35 y luego a 0.4, los índices de silueta mejoraron y el número de clusters se estabilizó en configuraciones con mejor cohesión y separación.

El mejor índice de silueta, de 0.341, se obtuvo con eps de 0.45 y min_samples entre 5 y 10, generando 2 clusters. Esto indica que estas configuraciones logran una agrupación moderada en la que los puntos están bien diferenciados entre los clusters. Con eps mayor a 0.45, el modelo no pudo formar más de un cluster, indicando que los datos se ven como un solo grupo a mayores distancias.

En conclusión, una configuración de eps de 0.45 con min_samples de 5 a 10 parece ser la mejor opción para DBSCAN, generando dos clusters bien separados sin perder demasiada información.

##Meanshift
"""

from sklearn.cluster import MeanShift
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler

# Aplicar Mean Shift al dataset normalizado
meanshift = MeanShift()
clusters_meanshift = meanshift.fit_predict(X_train)

# Calcular el índice de silueta para el clustering de Mean Shift
silhouette_meanshift = silhouette_score(X_train, clusters_meanshift)
print(f"Índice de silueta para el dataset original normalizado con Mean Shift: {silhouette_meanshift}")

# Visualización de los clusters en 2D usando PCA
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reducir el dataset a 2 dimensiones usando PCA para visualización
pca = PCA(n_components=2)
train_scaled_pca = pca.fit_transform(X_train)

# Graficar los clusters de Mean Shift
plt.figure(figsize=(10, 7))
plt.scatter(train_scaled_pca[:, 0], train_scaled_pca[:, 1], c=clusters_meanshift, cmap='viridis', marker='o', s=50)
plt.title("Visualización de Clusters en el Dataset Original Normalizado (Mean Shift)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Cluster")
plt.show()

"""El clustering de Mean Shift con el dataset normalizado ha mostrado un índice de silueta de 0.368, lo cual es un valor moderadamente bueno que sugiere una separación relativamente clara entre los clusters formados. Esto indica que Mean Shift ha logrado identificar grupos con cohesión interna razonable en comparación con otras técnicas probadas. Aunque el resultado no implica una separación perfecta, el valor de silueta es uno de los más altos entre los métodos utilizados, lo que posiciona a Mean Shift como una opción sólida para el agrupamiento en este caso.

---
# Conclusiones
---

**Discusión final de los resultados:**

Después de explorar y comparar varios algoritmos de clustering, incluyendo K-means, Clustering Jerárquico, DBSCAN y Mean Shift, he identificado diferencias significativas en su rendimiento en función de la cohesión y separación de los clusters. Cada algoritmo aportó una perspectiva diferente sobre la estructura del dataset, lo cual fue útil para evaluar la calidad del agrupamiento.

1. **K-means**: La configuración de 2 clusters en K-means obtuvo un índice de silueta de 0.37, sugiriendo que es la más adecuada para este dataset. Sin embargo, al aumentar el número de clusters, la cohesión disminuyó, lo que muestra una calidad de clustering moderada en general.

2. **Mean Shift**: Con un índice de silueta de 0.368, Mean Shift se destacó entre los algoritmos, mostrando una buena cohesión y separación de clusters sin requerir un número específico de clusters. Este valor de silueta relativamente alto indica que los puntos están bien agrupados internamente y diferenciados entre clusters.

3. **Clustering Jerárquico**: En el clustering jerárquico, el enlace completo (linkage 'complete'), de 3 clusters, logró un índice de silueta de 0.2968, el más alto entre los métodos jerárquicos probados. Sin embargo, el índice sigue siendo bajo, lo que indica que este método tiene dificultades para generar clusters bien diferenciados en este dataset.

4. **DBSCAN**: DBSCAN alcanzó su mejor índice de silueta (0.341) con un eps de 0.45 y min_samples entre 5 y 10, generando 2 clusters con una cohesión moderada. Este resultado destaca su capacidad para identificar clusters en zonas densas y detectar outliers. Sin embargo, configuraciones más altas de eps generaron un solo cluster, mostrando limitaciones cuando se requiere una mayor separación.

**Selección final del algoritmo y variables**: Considerando el índice de silueta y la naturaleza de los datos, he seleccionado **K-Means** como la opción final. Este algoritmo muestra una ventaja en la detección de estructuras densas y obtiene el índice de silueta más alto, reflejando una buena cohesión interna de los clusters. Para una futura tarea de predicción de contagios, variables meteorológicas como temperatura, humedad y precipitación son esenciales y han sido priorizadas en este análisis, mientras que otras variables de menor relevancia en la cohesión de los clusters pueden omitirse en futuros modelos supervisados.

**Posible reutilización en aprendizaje supervisado**: Algunos aspectos de este análisis de clustering, como la selección de variables y la normalización, serán útiles para futuras tareas supervisadas. Aunque el agrupamiento mismo no se reutilizaría directamente, los insights sobre las relaciones entre variables ayudarán a formular hipótesis y a estructurar modelos supervisados.

**Trabajo futuro**: Como parte del trabajo futuro, podría ser beneficioso realizar un análisis de sensibilidad para ajustar mejor los parámetros de clustering en algoritmos como DBSCAN y Mean Shift. Adicionalmente, explorar modelos supervisados basados en las variables seleccionadas para predecir tendencias de contagio en función del clima, incluyendo la evaluación de técnicas de validación cruzada, sería un paso importante para extender este análisis.

##Referencias

DrivenData. (2016). DengAI: Predicting Disease Spread. Retrieved [Month Day Year] from https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/.

OpenAI. (2024). ChatGPT: Asistente de inteligencia artificial para análisis de datos y aprendizaje supervisado. OpenAI.

Google AI. (2024). Gemini: An AI assistant for Google Colab.
"""